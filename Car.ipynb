{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#conver X_train and X_test to numerical values instead of string\n",
    "def convert(df):\n",
    "    buying_dict = {'low':1, 'med': 2, 'high':3, 'vhigh':4}\n",
    "    lug_dict = {'small':1, 'med':2, 'big':3}\n",
    "    safety_dict = {'low': 1, 'med':2, 'high':3}\n",
    "    doors_dict = {'1':1, '2':2, '3':3, '4':4, '5more': 5}\n",
    "    persons_dict = { '2':2, '4':4, 'more': 6}\n",
    "    df['buying'] = df['buying'].map(buying_dict)\n",
    "    df['maint'] = df['maint'].map(buying_dict)\n",
    "    df['lug_boot'] = df['lug_boot'].map(lug_dict)\n",
    "    df['safety'] = df['safety'].map(safety_dict)\n",
    "    df['doors'] = df['doors'].map(doors_dict)\n",
    "    df['persons'] = df['persons'].map(persons_dict)\n",
    "    return(df)\n",
    "\n",
    "def entropy(y):\n",
    "    #y[1,1,2,3]\n",
    "    #counts the occurrence of each element\n",
    "    hist = np.bincount(y)\n",
    "    #[1,2...8]\n",
    "    ps = hist / len(y)\n",
    "    #apply formula for entropy\n",
    "    ent =-np.sum([p * np.log2(p) for p in ps if p > 0])\n",
    "    return ent\n",
    "\n",
    "#class for Node\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "#leaf nodes are nodes right at the end (when decisions are made)\n",
    "    def is_leaf(self):\n",
    "        return self.value is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class for Decision Trees\n",
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, min_samples_split=2, max_depth=100, Num_features=None):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.Num_features = Num_features\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        #shape[1] = nuber of columns\n",
    "        self.Num_features = X.shape[1] if not self.Num_features else min(self.Num_features, X.shape[1])\n",
    "        self.root = self.grow(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def grow(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        #gives number of possible outcomes\n",
    "        labels = len(np.unique(y))\n",
    "\n",
    "        # stopping criteria\n",
    "        if (depth >= self.max_depth\n",
    "                or labels == 1\n",
    "                or n_samples < self.min_samples_split):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        #pick random feature\n",
    "        f_inds = np.random.choice(n_features, self.Num_features, replace=False)\n",
    "\n",
    "        # select the best split using IG values\n",
    "        best_feat, best_thresh = self._best_criteria(X, y, f_inds)\n",
    "        \n",
    "         # grow nodes from split\n",
    "        li, ri = self._split(X[:, best_feat], best_thresh)\n",
    "        left = self.grow(X[li, :], y[li], depth+1)\n",
    "        right = self.grow(X[ri, :], y[ri], depth+1)\n",
    "        return Node(best_feat, best_thresh, left, right)\n",
    "    \n",
    "#function to find the best threshold to use next for maximum information gain\n",
    "    def _best_criteria(self, X, y, f_inds):\n",
    "        best_gain = -1\n",
    "        split_idx, split_thresh = None, None\n",
    "        for f_ind in f_inds:\n",
    "            X_column = X[:, f_ind]\n",
    "            thresholds = np.unique(X_column)\n",
    "            #[1,,2,3,4,5,6]\n",
    "            for threshold in thresholds:\n",
    "                gain = self._information_gain(y, X_column, threshold)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = f_ind\n",
    "                    split_thresh = threshold\n",
    "\n",
    "        return split_idx, split_thresh\n",
    "\n",
    "    def _information_gain(self, y, X_column, split_thresh):\n",
    "        # parent loss\n",
    "        parent_entropy = entropy(y)\n",
    "        # generate split\n",
    "        li, ri = self._split(X_column, split_thresh)\n",
    "\n",
    "        if len(li) == 0 or len(ri) == 0:\n",
    "            return 0\n",
    "\n",
    "        n = len(y)\n",
    "        no_left, no_right = len(li), len(ri)\n",
    "        ent_l, ent_r = entropy(y[li]), entropy(y[ri])\n",
    "        child_entropy = (no_left / n) * ent_l + (no_right / n) * ent_r\n",
    "\n",
    "        # information gain is difference in loss before vs. after split\n",
    "        ig = parent_entropy - child_entropy\n",
    "        return ig\n",
    "#function to split the indices to he ones les than and more tan the threshold\n",
    "    def _split(self, X_column, split_thresh):\n",
    "        li = np.argwhere(X_column <= split_thresh).flatten()\n",
    "        ri = np.argwhere(X_column > split_thresh).flatten()\n",
    "        return li, ri\n",
    "#function to traverse down the tree\n",
    "    def _traverse_tree(self, x, node):\n",
    "        #we've reached the end of the tree, ths return a value\n",
    "        if node.is_leaf():\n",
    "            return node.value\n",
    "\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "    \n",
    "#used to find most common data type after split\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        most_common = counter.most_common(1)[0][0]\n",
    "        return most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_label(y):\n",
    "    counter = Counter(y)\n",
    "    most_common = counter.most_common(1)[0][0]\n",
    "    return most_common\n",
    "\n",
    "def pick_data(X, y):\n",
    "    n_samples = X.shape[0]\n",
    "    idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
    "    return X[idxs], y[idxs]\n",
    "\n",
    "\n",
    "#Class for Random Forest\n",
    "class RandomForest:\n",
    "    \n",
    "    def __init__(self, n_trees=8, min_samples_split=1,\n",
    "                 max_depth=100, Num_features=None):\n",
    "        self.n_trees = n_trees\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.Num_features = Num_features\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        for _ in range(self.n_trees):\n",
    "            tree = DecisionTree(min_samples_split=self.min_samples_split,\n",
    "                max_depth=self.max_depth, Num_features=self.Num_features)\n",
    "            X_samp, y_samp = pick_data(X, y)\n",
    "            tree.fit(X_samp, y_samp)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        predictions = np.swapaxes(predictions, 0, 1)\n",
    "        y_pred = [most_common_label(tree_pred) for tree_pred in predictions]\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95761078998073212"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def accuracy(y_true, y_predicted):\n",
    "    correct = np.sum(y_true == y_predicted) \n",
    "    accuracy = correct/ len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "X_train = (convert(pd.read_csv(\"dataset_files/car_X_train.csv\", header=0))).to_numpy()\n",
    "X_test = (convert(pd.read_csv(\"dataset_files/car_X_test.csv\", header=0))).to_numpy()\n",
    "\n",
    "y_dict={ 'unacc':1, 'acc':2 ,'good':3, 'vgood':4}\n",
    "\n",
    "y_train = pd.read_csv(\"dataset_files/car_y_train.csv\", header=0)\n",
    "y_train['class'] = y_train['class'].map(y_dict)\n",
    "y_train = y_train.to_numpy()\n",
    "y_train=y_train.flatten()\n",
    "\n",
    "y_test = pd.read_csv(\"dataset_files/car_y_test.csv\", header=0)\n",
    "y_test['class'] = y_test['class'].map(y_dict)\n",
    "y_test = y_test.to_numpy()\n",
    "y_test=y_test.flatten()\n",
    "\n",
    "clf = RandomForest(n_trees=7, max_depth=10)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy(y_test, y_pred)\n",
    "df3 = acc.view()\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
